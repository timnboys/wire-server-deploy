# PostgreSQL High Availability Cluster Deployment Guide

## Table of Contents
- [Architecture Overview](#architecture-overview)
- [Key Concepts](#key-concepts)
- [High Availability Features](#high-availability-features)
- [Inventory Definition](#inventory-definition)
- [Installation Process](#i### üõ°Ô∏è Automated Split-Brain Monitoring

The system deploys a comprehensive monitoring solution that includes:allation-process)
- [Monitoring Checks After Installation](#monitoring-checks-after-installation)
- [How It Confirms a Reliable System](#how-it-confirms-a-reliable-system)
- [Node Recovery Operations](#node-recovery-operations)
- [Wire Server Database Setup](#wire-server-database-setup)

## Architecture Overview

The PostgreSQL cluster implements a **Primary-Replica High Availability** architecture with intelligent **split-brain protection** and **automatic failover capabilities**:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   PostgreSQL1   ‚îÇ    ‚îÇ   PostgreSQL2   ‚îÇ    ‚îÇ   PostgreSQL3   ‚îÇ
‚îÇ    (Primary)    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   (Replica)     ‚îÇ    ‚îÇ   (Replica)     ‚îÇ
‚îÇ   Read/Write    ‚îÇ    ‚îÇ   Read-Only     ‚îÇ    ‚îÇ   Read-Only     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ PostgreSQL 17 ‚îÇ    ‚îÇ ‚Ä¢ PostgreSQL 17 ‚îÇ    ‚îÇ ‚Ä¢ PostgreSQL 17 ‚îÇ
‚îÇ ‚Ä¢ repmgr        ‚îÇ    ‚îÇ ‚Ä¢ repmgr        ‚îÇ    ‚îÇ ‚Ä¢ repmgr        ‚îÇ
‚îÇ ‚Ä¢ repmgrd       ‚îÇ    ‚îÇ ‚Ä¢ repmgrd       ‚îÇ    ‚îÇ ‚Ä¢ repmgrd       ‚îÇ
‚îÇ ‚Ä¢ Split-brain   ‚îÇ    ‚îÇ ‚Ä¢ Split-brain   ‚îÇ    ‚îÇ ‚Ä¢ Split-brain   ‚îÇ
‚îÇ   monitoring    ‚îÇ    ‚îÇ   monitoring    ‚îÇ    ‚îÇ   monitoring    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                       ‚îÇ                       ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ Intelligent     ‚îÇ
                    ‚îÇ ‚Ä¢ Failover      ‚îÇ
                    ‚îÇ ‚Ä¢ Split-brain   ‚îÇ
                    ‚îÇ   Protection    ‚îÇ
                    ‚îÇ ‚Ä¢ Self-healing  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

1. **PostgreSQL 17 Cluster**: Latest stable PostgreSQL with performance improvements
2. **repmgr**: Cluster management and automatic failover orchestration
3. **Split-Brain Detection**: Intelligent monitoring prevents data corruption scenarios
4. **Event-Driven Recovery**: Automatic handling of cluster state changes
5. **Wire-Server Integration**: Pre-configured for Wire backend services

## Key Concepts

### Streaming Replication
- **Real-time Data Sync**: WAL records streamed from primary to replicas
- **Hot Standby**: Replicas accept read-only queries during replication
- **Physical Replication Slots**: Ensure WAL retention for disconnected replicas
- **Asynchronous Mode**: Optimized for performance over strict consistency

### Cluster Management
- **repmgr**: PostgreSQL cluster management and failover tool
- **repmgrd**: Background daemon monitoring cluster health
- **Event Notifications**: Custom scripts respond to cluster state changes
- **Metadata Tracking**: Comprehensive cluster state information

### Split-Brain Protection
- **Detection Logic**: Monitors for multiple primary scenarios
- **Automatic Protection**: Masks services to prevent data corruption
- **Self-Healing**: Automatic recovery from resolved conflicts
- **Manual Override**: Administrative control for complex scenarios

## High Availability Features

### üéØ Automatic Failover Process

```mermaid
graph TD
    A[Primary Node Failure] --> B[repmgrd Detects Failure]
    B --> C[Promote Best Replica]
    C --> D[Update Cluster Metadata]
    D --> E[Redirect Traffic to New Primary]
    E --> F[Event Notification]
    F --> G[Fence Script Updates Status]
```

#### **1. Failure Detection**
- **Health Monitoring**: repmgrd continuously monitors primary connectivity
- **Configurable Timeouts**: Customizable failure detection intervals
- **Network Partition Handling**: Distinguishes between node and network failures

#### **2. Automatic Promotion**
- **Best Candidate Selection**: Promotes replica with most recent data
- **Timeline Management**: Handles PostgreSQL timeline advancement
- **Slot Management**: Updates replication slot configurations

#### **3. Cluster Reconfiguration**
- **Automatic Rewiring**: Remaining replicas connect to new primary
- **Connection Updates**: Applications automatically redirect to new primary
- **Metadata Sync**: repmgr cluster metadata updated consistently

### üõ°Ô∏è Split-Brain Protection

#### **Detection Algorithm**
```bash
# Intelligent Split-Brain Detection
1. Check: Am I a primary? (pg_is_in_recovery() = false)
2. Check: Do I have active replicas? (pg_stat_replication count = 0)
3. If isolated ‚Üí Query other cluster nodes for primary status
4. If another node is also primary ‚Üí SPLIT-BRAIN DETECTED
```

#### **Protection Response**
```bash
# Automatic Protection Sequence
1. MASK PostgreSQL service (prevents restart attempts)
2. STOP PostgreSQL service (stops conflicting writes)
3. VERIFY service stopped (force kill if needed)
4. LOG incident (comprehensive audit trail)
5. ALERT administrators (split-brain detected)
```

#### **Recovery Automation**
- **Event-Driven**: Fence script responds to cluster events
- **Auto-Unmask**: Service unmasked during successful rejoins
- **State Tracking**: Comprehensive logging and status updates

### üîÑ Self-Healing Capabilities

#### **Automatic Recovery Scenarios**
| Scenario | Detection Time | Recovery Action | Data Loss Risk |
|----------|----------------|-----------------|----------------|
| Primary Hardware Failure | 5-30 seconds | Automatic Promotion | None (with sync replication) |
| Network Partition | 30-60 seconds | Split-brain Protection | None (writes stopped) |
| Replica Failure | Immediate | Continue with remaining replicas | None |
| Service Restart | Immediate | Automatic reconnection | None |

#### **Manual Recovery Support**
- **Guided Procedures**: Clear recovery commands and procedures
- **Diagnostic Tools**: Comprehensive status and health checks
- **Rollback Capability**: Safe recovery from failed operations

## Inventory Definition

The PostgreSQL cluster requires a properly structured inventory to define node roles and configuration. The inventory file should be located at `ansible/inventory/offline/hosts.ini` or your specific environment path.

### Inventory Structure

```ini
[all]
postgresql1 ansible_host=192.168.122.236
postgresql2 ansible_host=192.168.122.233
postgresql3 ansible_host=192.168.122.206

[postgresql:vars]
postgresql_network_interface = enp1s0
postgresql_version = 17
wire_dbname = wire-server
wire_user = wire-server
# Optional: wire_pass = verysecurepassword (if not defined, auto-generated)

# All PostgreSQL nodes
[postgresql]
postgresql1
postgresql2
postgresql3

# Primary (read-write) node
[postgresql_rw]
postgresql1

# Replica (read-only) nodes
[postgresql_ro]
postgresql2
postgresql3
```

### Node Groups Explained

| Group | Purpose | Nodes | Role |
|-------|---------|-------|------|
| `postgresql` | All PostgreSQL nodes | postgresql1-3 | Base configuration |
| `postgresql_rw` | Primary nodes | postgresql1 | Read/Write operations |
| `postgresql_ro` | Replica nodes | postgresql2-3 | Read-only operations |

### Configuration Variables

| Variable | Default | Description | Required |
|----------|---------|-------------|----------|
| `postgresql_network_interface` | `enp1s0` | Network interface for cluster communication | No |
| `postgresql_version` | `17` | PostgreSQL major version | No |
| `wire_dbname` | `wire-server` | Database name for Wire application | Yes |
| `wire_user` | `wire-server` | Database user for Wire application | Yes |
| `wire_pass` | auto-generated | Password (displayed after deployment) | No |

## Installation Process

### üöÄ Complete Installation (Fresh Deployment)

#### **Prerequisites**
- Ubuntu 20.04+ or Debian 11+ on all nodes
- Minimum 4GB RAM per node (8GB+ recommended)
- SSH access configured for Ansible with sudo privileges
- Network connectivity between all nodes (PostgreSQL port 5432)
- Firewall configured to allow PostgreSQL traffic between nodes

#### **Step 1: Verify Connectivity**
```bash
# Test Ansible connectivity to all nodes
ansible all -i ansible/inventory/offline/hosts.ini -m ping
```

#### **Step 2: Full Cluster Deployment**
```bash
# Deploy complete PostgreSQL HA cluster
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml
```

**‚è±Ô∏è Expected Duration: 10-15 minutes**

This single command performs:
1. ‚úÖ **Package Installation**: PostgreSQL 17 + repmgr + dependencies
2. ‚úÖ **Primary Setup**: Configure primary node with repmgr database
3. ‚úÖ **Replica Deployment**: Clone and configure replica nodes
4. ‚úÖ **Verification**: Health checks and replication status
5. ‚úÖ **Wire Integration**: Create Wire database and user
6. ‚úÖ **Monitoring**: Deploy split-brain detection system

#### **Step 3: Verify Installation**
See the [Monitoring Checks](#monitoring-checks-after-installation) section for comprehensive verification procedures.

### üéØ Selective Installation (Using Tags)

The deployment supports granular control through tags for partial deployments or troubleshooting.

#### **Available Tags**

| Tag | Component | Description |
|-----|-----------|-------------|
| `cleanup` | Pre-deployment | Clean previous deployment state |
| `install` | Package Installation | Install PostgreSQL and dependencies |
| `primary` | Primary Node | Configure primary PostgreSQL node |
| `replica` | Replica Nodes | Configure replica PostgreSQL nodes |
| `verify` | Health Checks | Verify cluster health and replication |
| `monitoring` | Split-brain Detection | Deploy monitoring system |
| `wire-setup` | Wire Integration | Create Wire database and user |

#### **Tag-Based Installation Examples**

**Install packages only:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "install"
```

**Deploy primary node only:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "primary"
```

**Deploy primary and replicas (skip installation):**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "primary,replica"
```

**Run only verification:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "verify"
```

#### **Skip Tags for Partial Deployments**

```bash
# Resume from replica deployment (if primary is already configured)
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --skip-tags "cleanup,install,primary"

# Deploy basic cluster without split-brain monitoring
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --skip-tags "monitoring"

# Deploy cluster without Wire-specific database setup
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --skip-tags "wire-setup"
```

#### **Common Deployment Scenarios**

```bash
# If deployment failed during replica setup
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --skip-tags "cleanup,install,primary"

# Deploy monitoring on pre-existing cluster
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "monitoring"

# Clean up previous deployment state first
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-playbooks/clean_exiting_setup.yml
# Then run fresh deployment
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml
```

## Monitoring Checks After Installation

The deployment includes an intelligent monitoring system that continuously watches for split-brain scenarios and automatically protects the cluster from data corruption.

### ÔøΩÔ∏è Automated Split-Brain Monitoring

The system deploys a comprehensive monitoring solution that includes:

#### **1. systemd Timer-Based Monitoring**
```bash
# Split-brain detection timer (runs every 30 seconds)
systemctl status detect-rouge-primary.timer

# Split-brain detection service
systemctl status detect-rouge-primary.service
```

#### **2. Monitoring Script Location**
```bash
# The monitoring script is deployed to:
/usr/local/bin/detect_rouge_primary.sh

# Manual execution for testing:
sudo -u postgres /usr/local/bin/detect_rouge_primary.sh
```

#### **3. Event-Driven Fence Script**
```bash
# Fence script handles repmgr events:
/usr/local/bin/simple_fence.sh

# This script responds to:
# - repmgr_failover_promote (new primary promotion)
# - repmgr_failover_follow (replica following new primary)
# - node_rejoin (node rejoining cluster)
```

### üîç Manual Verification Commands

#### **Cluster Status Overview**
```bash
# Primary command to verify cluster health
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf cluster show

# Expected healthy output:
# ID | Name        | Role    | Status    | Upstream    | Location | Priority | Timeline
#----+-------------+---------+-----------+-------------+----------+----------+----------
# 1  | postgresql1 | primary | * running |             | default  | 100      | 1
# 2  | postgresql2 | standby |   running | postgresql1 | default  | 100      | 1
# 3  | postgresql3 | standby |   running | postgresql1 | default  | 100      | 1
```

#### **Replication Health Verification**
```bash
# Check streaming replication status (run on primary)
sudo -u postgres psql -c "
SELECT
    application_name,
    client_addr,
    state,
    sync_state,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), replay_lsn)) as lag_size
FROM pg_stat_replication;
"

# Verify replication slots are active (run on primary)
sudo -u postgres psql -c "
SELECT
    slot_name,
    active,
    pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) as slot_lag
FROM pg_replication_slots;
"
```

#### **Service Status Verification**
```bash
# Check all PostgreSQL-related services on each node
systemctl status postgresql@17-main.service      # PostgreSQL server
systemctl status repmgrd@17-main.service         # repmgr daemon
systemctl status detect-rouge-primary.timer      # Split-brain monitoring timer

# All should show "active (running)" status
```

### üìä Monitoring System Details

#### **What the Monitoring Does**
1. **Isolation Detection**: Checks if current node is an isolated primary (no active replicas)
2. **Cross-Node Verification**: Queries other cluster nodes to detect multiple primaries
3. **Automatic Protection**: Masks and stops PostgreSQL service when split-brain detected
4. **Event Logging**: Comprehensive logging of all monitoring activities
5. **Self-Healing**: Automatic service unmasking during successful rejoins

#### **Monitoring Frequency**
- **Detection Interval**: Every 30 seconds via systemd timer
- **Event Response**: Immediate via repmgr event notifications

#### **Log Locations**
```bash
# Split-brain monitoring logs
journalctl -u detect-rouge-primary.service --since "1 hour ago"

# repmgr event logs
journalctl -u repmgrd@17-main.service --since "1 hour ago"

# General PostgreSQL logs
journalctl -u postgresql@17-main.service --since "1 hour ago"
```

### üéØ Key Monitoring Verification Steps

#### **Verify Split-Brain Monitoring is Active**
```bash
# Check timer is running
systemctl is-active detect-rouge-primary.timer

# Check recent execution
journalctl -u detect-rouge-primary.service --since "5 minutes ago" | tail -10
```

#### **Verify repmgr Monitoring**
```bash
# Check recent repmgr events
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf cluster event --limit=5
```

#### **Test Monitoring Response**
```bash
# The monitoring system will automatically:
# 1. Detect if this node becomes isolated primary
# 2. Query other nodes for primary status
# 3. Mask and stop services if split-brain detected
# 4. Log all actions for audit trail

# Check monitoring configuration
cat /etc/systemd/system/detect-rouge-primary.timer
cat /etc/systemd/system/detect-rouge-primary.service
```

## How It Confirms a Reliable System

### üõ°Ô∏è Built-in Reliability Features

The PostgreSQL HA deployment includes several layers of reliability confirmation through automated monitoring and protection mechanisms:

#### **1. Split-Brain Prevention**
The system automatically prevents split-brain scenarios through:
- **Intelligent Detection**: Every 30 seconds, checks for isolated primary conditions
- **Cross-Node Verification**: Queries other cluster nodes to detect multiple primaries
- **Automatic Protection**: Masks and stops services when dangerous conditions detected
- **Event Logging**: Comprehensive audit trail of all protection actions

#### **2. Automatic Failover Reliability**
```bash
# repmgr provides automatic failover with:
# - Health monitoring of primary node
# - Automatic promotion of best replica candidate
# - Cluster metadata updates
# - Connection redirection to new primary
```

#### **3. Data Consistency Verification**
The system ensures data consistency through:
- **Streaming Replication**: Real-time WAL record synchronization
- **Replication Slots**: Prevent WAL deletion for disconnected replicas
- **Timeline Management**: PostgreSQL handles timeline advancement automatically
- **Automatic Rewinding**: pg_rewind handles timeline divergences during rejoin

### üéØ Reliability Verification Methods

#### **Cluster Health Indicators**
You can verify system reliability by checking these indicators:

```bash
# 1. All nodes show proper status
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf cluster show
# Expected: One primary "* running", all replicas "running"

# 2. Replication is active and synchronized
sudo -u postgres psql -c "SELECT * FROM pg_stat_replication;"
# Expected: Shows connected replicas with minimal lag

# 3. Split-brain monitoring is active
systemctl status detect-rouge-primary.timer
# Expected: "active (waiting)" status

# 4. All services are running properly
systemctl status postgresql@17-main repmgrd@17-main
# Expected: All services "active (running)"
```

#### **Failover Testing Results**
The system's reliability can be confirmed through controlled failover testing:
- **Failover Detection**: < 30 seconds to detect primary failure
- **Automatic Promotion**: < 30 seconds to promote new primary
- **Service Recovery**: < 2 minutes for complete cluster stabilization
- **Data Consistency**: Zero data loss with proper replication setup

#### **Recovery Capabilities**
The system demonstrates reliability through automated recovery:
- **Node Rejoin**: Automatic rejoining of failed nodes when they recover
- **Split-Brain Recovery**: Automatic service restoration after split-brain resolution
- **Timeline Handling**: Automatic data synchronization using pg_rewind
- **Service Management**: Automatic unmasking of services during successful operations

### üìä System Reliability Metrics

#### **Availability Targets**
- **Cluster Uptime**: 99.9%+ with proper maintenance windows
- **Failover Time**: < 30 seconds automatic detection and promotion
- **Recovery Time**: < 2 minutes for node rejoin operations
- **Data Protection**: 100% split-brain detection and prevention

#### **Monitoring Coverage**
- **Continuous Monitoring**: 24/7 automated split-brain detection
- **Event-Driven Response**: Immediate reaction to cluster state changes
- **Comprehensive Logging**: Full audit trail of all cluster operations
- **Health Verification**: Regular replication status and lag monitoring

The reliability of the system is confirmed through these automated processes working together to maintain cluster integrity, prevent data corruption, and ensure high availability of the PostgreSQL service.

## Node Recovery Operations

### üîÑ Manual Node Rejoin

When a node needs to rejoin the cluster after being disconnected or failed:

#### **Standard Rejoin Command**
```bash
# Standard rejoin (when data is still compatible)
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node rejoin \
    -d repmgr -h <primary-ip> -U repmgr --verbose
```

#### **Force Rejoin with Rewind**
```bash
# Force rejoin with pg_rewind (when timelines diverged)
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node rejoin \
    -d repmgr -h <primary-ip> -U repmgr --force-rewind --verbose
```

#### **Complete Node Recovery Process**

**Step 1: Assess Node State**
```bash
# Check if service is masked (from split-brain protection)
systemctl is-enabled postgresql@17-main.service

# If output is "masked", unmask first:
sudo systemctl unmask postgresql@17-main.service
```

**Step 2: Check Data Compatibility**
```bash
# Check current node status
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node status

# Check primary connectivity
sudo -u postgres psql -h <primary-ip> -U repmgr -d repmgr -c "SELECT 1;"
```

**Step 3: Execute Rejoin**
```bash
# For split-brain recovery or timeline divergence:
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node rejoin \
    -d repmgr -h <primary-ip> -U repmgr --force-rewind --verbose

# Monitor rejoin progress:
# - WAL files being synchronized
# - Connection establishment
# - Replication startup confirmation
```

**Step 4: Verify Recovery**
```bash
# Confirm node is back in cluster
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf cluster show

# Check replication is active
sudo -u postgres psql -c "SELECT pg_is_in_recovery();" # Should return 't' for replica

# Verify data synchronization
sudo -u postgres psql -c "SELECT pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn();"
```

### üö® Emergency Recovery Scenarios

#### **Complete Cluster Failure Recovery**
```bash
# 1. Identify node with most recent data
for node in postgresql1 postgresql2 postgresql3; do
    echo "=== $node ==="
    ssh $node "sudo -u postgres pg_controldata /var/lib/postgresql/17/main | grep 'Latest checkpoint'"
done

# 2. Start PostgreSQL on best candidate
sudo systemctl unmask postgresql@17-main.service
sudo systemctl start postgresql@17-main.service

# 3. Register as new primary
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf primary register --force

# 4. Rejoin other nodes as replicas
# On each remaining node:
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node rejoin \
    -d repmgr -h <new-primary-ip> -U repmgr --force-rewind --verbose
```

#### **Split-Brain Resolution**
```bash
# 1. Identify which node should remain primary
# Check data consistency, application connections, etc.

# 2. On the node that should become replica:
sudo systemctl unmask postgresql@17-main.service
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf node rejoin \
    -d repmgr -h <correct-primary-ip> -U repmgr --force-rewind --verbose

# 3. Verify split-brain is resolved
sudo -u postgres repmgr -f /etc/repmgr/17-main/repmgr.conf cluster show
# Should show only one primary
```

## Wire Server Database Setup

### Wire Server Database Setup

The [`postgresql-wire-setup.yml`](../ansible/postgresql-playbooks/postgresql-wire-setup.yml) playbook creates the dedicated database and user account required for Wire server operation.

#### Overview
This playbook runs exclusively on the primary PostgreSQL node (`postgresql_rw` group) and performs:

1. **Database Management**: Creates the Wire server database if it doesn't exist
2. **User Account Management**: Creates Wire user with secure password generation
3. **Credential Display**: Shows generated credentials for Wire server configuration

#### Usage

**Run independently:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-playbooks/postgresql-wire-setup.yml
```

**Run as part of main deployment:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --tags "wire-setup"
```

**Skip during main deployment:**
```bash
ansible-playbook -i ansible/inventory/offline/hosts.ini ansible/postgresql-deploy.yml --skip-tags "wire-setup"
```

#### Important Notes
- **Credential Security**: Generated password is displayed in Ansible output
- **Save Credentials**: Store password securely for Wire server configuration
- **One-Time Setup**: Run only once per cluster deployment
